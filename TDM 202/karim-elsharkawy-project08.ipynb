{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be02a957-7133-4d02-818e-fedeb3cecb05",
   "metadata": {},
   "source": [
    "# Project 8 -- [Karim] [El-Sharkawy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1228853-dd19-4ab2-89e0-0394d7d72de3",
   "metadata": {},
   "source": [
    "**TA Help:** No help :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180e742-8e39-4698-98ff-5b00c8cf8ea0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c8eb9-2513-4946-98c8-f1bc5600822c",
   "metadata": {},
   "source": [
    "**A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49445606-d363-41b4-b479-e319a9a84c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 03:25:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/04 03:25:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create the Session\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"PySpark Tutorial\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96322b50-c317-4117-bb61-386397f63d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.703458309173584\n"
     ]
    }
   ],
   "source": [
    "# Generate a random dataframe with 10^7 rows\n",
    "data = {'col1':np.random.randint(1,10,10000000),'col2':np.random.randint(1,10,10000000)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating our pandas dataframe\n",
    "pandasDF = df.copy()\n",
    "\n",
    "startPandas = time.time()\n",
    "pandasDF['col2'] = pandasDF['col2'].map(lambda x: x - (x*10/100))\n",
    "endPandas = time.time()\n",
    "print(endPandas - startPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44d81d2-322b-4e10-bdfc-d453648d810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n",
      "/usr/local/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n"
     ]
    }
   ],
   "source": [
    "# Creating our spark dataframe\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "sparkDF = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f085c674-665b-4ed3-9be6-10348b262890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5703299045562744\n"
     ]
    }
   ],
   "source": [
    "startSpark = time.time()\n",
    "sparkRDD = sparkDF.rdd.map(lambda x: [x[0],x[1] - (x[1]*10/100)])\n",
    "endSpark = time.time()\n",
    "print(endSpark - startSpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3bdb3-dc2e-4fc8-8d67-14363e676f07",
   "metadata": {},
   "source": [
    "**B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456e57c-4a12-464b-999a-ef2df5af80c1",
   "metadata": {},
   "source": [
    "This lab is introducing us and getting us used to PySpark. Spark is at least 20 times faster than pandas with processing large amounts of data (I'm using 2 cores). I'm assuming this speed difference isn't as large when we're using small data sets, but this is good to know because now I'll just use Spark instead of PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc601975-35ed-4680-a4e1-0273ee3cc047",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16336a1-1ef0-41e8-bc7c-49387db27497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 03:25:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "# creating a spark application named TDM_S that uses 2 gigabytes of the spark driver program\n",
    "sp = SparkSession.builder.appName('TDM_S').config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "#.getOrCreate() actually creates the spark sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f48c2e4-86f4-42ec-8c9a-8f651b75d945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 03:25:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "|station_id|latitude|longitude|           name|    observation_time|temperature|temperature_high|temperature_low|humidity|solar_radiation|solar_radiation_high|rain|rain_inches_last_hour|wind_speed_mph|wind_direction_degrees|wind_gust_speed_mph|wind_gust_direction_degrees|pressure|soil_temp_1|soil_temp_2|soil_temp_3|soil_temp_4|soil_moist_1|soil_moist_2|soil_moist_3|soil_moist_4|\n",
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-10T04:00:00Z|       70.0|            71.0|           70.0|    83.0|           NULL|                NULL| 0.0|                  0.0|           0.0|                  NULL|                3.0|                      247.5|   30.05|       77.0|       78.0|       76.0|       74.0|        24.0|        24.0|        10.0|         9.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-10T04:15:00Z|       69.0|            70.0|           69.0|    84.0|           NULL|                NULL| 0.0|                  0.0|           1.0|                 247.5|                3.0|                      247.5|   30.04|       76.0|       78.0|       76.0|       74.0|        24.0|        25.0|        10.0|         9.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:00:00Z|       76.0|            77.0|           76.0|    76.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 202.5|                4.0|                      202.5|   29.89|       80.0|       80.0|       78.0|       75.0|        31.0|        30.0|        12.0|        10.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:15:00Z|       76.0|            76.0|           76.0|    77.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 202.5|                4.0|                      202.5|   29.88|       80.0|       80.0|       78.0|       75.0|        31.0|        31.0|        12.0|        10.0|\n",
      "|         1|40.93894|-86.47418|WHIN001-PULA001|2019-07-11T04:30:00Z|       76.0|            76.0|           76.0|    77.0|           NULL|                NULL| 0.0|                  0.0|           2.0|                 225.0|                4.0|                      202.5|   29.88|       80.0|       80.0|       78.0|       75.0|        32.0|        31.0|        12.0|        10.0|\n",
      "+----------+--------+---------+---------------+--------------------+-----------+----------------+---------------+--------+---------------+--------------------+----+---------------------+--------------+----------------------+-------------------+---------------------------+--------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF = sp.read.parquet(\"/anvil/projects/tdm/data/whin/weather.parquet\")\n",
    "myDF.show(5) #Showing the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc22d4-ddc3-41cc-a91a-cb0025bc0c80",
   "metadata": {},
   "source": [
    "For this problem, we're using pyspark to create a dataframe and read in weather data from WHIN. We're then showing the first 5 rows of that dataframe. It's important to note that the first question being run might affect the output, however I haven't seen a difference between mine and the output in Dr. Ward's video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cae037-b7aa-4e4d-b8c7-632787384651",
   "metadata": {},
   "source": [
    "The WHIN dataset contains specific meteorological data from multiple stations around the US. Each station has a station ID and contain data on the temperature, dew point, rainfall, and mnay other things :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586edd-ff26-4ce2-8f6b-2424b26f2929",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe0f40d-9655-4653-9ca8-886bdb61cb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- observation_time: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- temperature_high: double (nullable = true)\n",
      " |-- temperature_low: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- solar_radiation: double (nullable = true)\n",
      " |-- solar_radiation_high: double (nullable = true)\n",
      " |-- rain: double (nullable = true)\n",
      " |-- rain_inches_last_hour: double (nullable = true)\n",
      " |-- wind_speed_mph: double (nullable = true)\n",
      " |-- wind_direction_degrees: double (nullable = true)\n",
      " |-- wind_gust_speed_mph: double (nullable = true)\n",
      " |-- wind_gust_direction_degrees: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- soil_temp_1: double (nullable = true)\n",
      " |-- soil_temp_2: double (nullable = true)\n",
      " |-- soil_temp_3: double (nullable = true)\n",
      " |-- soil_temp_4: double (nullable = true)\n",
      " |-- soil_moist_1: double (nullable = true)\n",
      " |-- soil_moist_2: double (nullable = true)\n",
      " |-- soil_moist_3: double (nullable = true)\n",
      " |-- soil_moist_4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "myDF.printSchema() #Listing column names and data types of the WHIN dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b452fb-a8d7-4f65-b3d6-b2cfa950d239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7494263"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B\n",
    "# couting and print the rows\n",
    "myDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a062794c-0202-46da-b982-13219b9d8678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#C\n",
    "myDF.select('station_id').distinct().count() # counting the number of stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6229f-35f7-400c-8366-c442baa5cf47",
   "metadata": {},
   "source": [
    "For part B, the .count() function is simply counting the number of rows, not actually displaying how many unique values there are. Part c is printing how many unique values of 'station_id' there are. In other words, how may stations have data provided in part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f29c-d245-4d2b-9fc1-ca14cb6087d9",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cffc767-d1c8-4d64-b7dc-f0d2ee8a80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A\n",
    "myDF.createOrReplaceTempView(\"weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a87387b-e5d7-46eb-a38c-c22a82b01c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|station_id|Total_Records|\n",
      "+----------+-------------+\n",
      "|       191|         7914|\n",
      "|        65|        69664|\n",
      "|        54|        57177|\n",
      "|       167|        27498|\n",
      "|       155|        43028|\n",
      "|       112|        68669|\n",
      "|       113|        71729|\n",
      "|       130|        44085|\n",
      "|        77|        32312|\n",
      "|       184|        13509|\n",
      "|       188|         9921|\n",
      "|       126|        69271|\n",
      "|       149|        44095|\n",
      "|        94|        29023|\n",
      "|        50|        65900|\n",
      "|       190|         7735|\n",
      "|       110|        71622|\n",
      "|       136|        71682|\n",
      "|        57|        71701|\n",
      "|       144|        45530|\n",
      "+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "# Get the total number of records for each station\n",
    "myresults = sp.sql(\"SELECT station_id, COUNT(*) AS Total_Records FROM weather GROUP BY station_id\")\n",
    "myresults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1181bac-ad7f-4e3f-8566-3e76b1fb1f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|station_id|Max_wind_speed|\n",
      "+----------+--------------+\n",
      "|       191|          18.0|\n",
      "|        65|          37.0|\n",
      "|        54|          32.0|\n",
      "|       167|          36.0|\n",
      "|       155|          32.0|\n",
      "|       112|          30.0|\n",
      "|       113|          26.0|\n",
      "|       130|          33.0|\n",
      "|        77|           5.0|\n",
      "|       184|          31.0|\n",
      "|       188|          23.0|\n",
      "|       126|          30.0|\n",
      "|       149|          27.0|\n",
      "|        94|          32.0|\n",
      "|        50|          12.0|\n",
      "|       190|          24.0|\n",
      "|       110|          32.0|\n",
      "|       136|          36.0|\n",
      "|        57|          27.0|\n",
      "|       144|          35.0|\n",
      "+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#C\n",
    "myresults = sp.sql(\"SELECT station_id, MAX(wind_speed_mph) AS Max_wind_speed FROM weather GROUP BY station_id\")\n",
    "myresults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb6ebb8b-afdb-429d-baaf-3bd73a5a100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|station_id|   avg_temperature|\n",
      "+----------+------------------+\n",
      "|       191| 69.37035633055345|\n",
      "|        65| 52.44688029402475|\n",
      "|        54|  53.7297479420121|\n",
      "|       167|47.787132673843466|\n",
      "|       155| 55.48980960364028|\n",
      "|       112| 52.58802370793225|\n",
      "|       113| 52.91307500313733|\n",
      "|       130| 55.45146875354429|\n",
      "|        77| 51.16190947928921|\n",
      "|       184| 60.51306536383152|\n",
      "|       188| 65.22620967741935|\n",
      "|       126| 52.58266807177607|\n",
      "|       149| 55.31502437918131|\n",
      "|        94|49.612962133480345|\n",
      "|        50| 51.33506828528073|\n",
      "|       190| 69.67356173238527|\n",
      "|       110|  52.8127391025104|\n",
      "|       136| 52.40440017299348|\n",
      "|        57| 52.13385540991387|\n",
      "|       144| 55.07742148034263|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#D\n",
    "myresults = sp.sql(\"SELECT station_id, AVG(temperature) AS avg_temperature FROM weather GROUP BY station_id\")\n",
    "myresults.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d552245-b4d6-474a-9cc9-fa7b8e674d55",
   "metadata": {},
   "source": [
    "We're creating SQL queries for parts B,C, and D. Basically, we're setting them up as tables so that we could see the data. For part B, we're seeing the stations' ID and how many records they keep. For part C, we're getting the max wind speed recorded for each station. For part D, we're getting the average temperature for each station.\n",
    "\n",
    "For all parts, only the top 20 stations show up, but this can be changed. There are 152 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9cdac-3e92-498f-83fa-e089bfc44ac8",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d370d7c9-06db-42b9-b75f-240481a5c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|station_id|avg_solar_radiation|\n",
      "+----------+-------------------+\n",
      "|       191| 219.90055597675007|\n",
      "|        65|               NULL|\n",
      "|        54|               NULL|\n",
      "|       167| 170.86899912714577|\n",
      "|       155|  177.7247611979603|\n",
      "|       112|               NULL|\n",
      "|       113|               NULL|\n",
      "|       130|               NULL|\n",
      "|        77|               NULL|\n",
      "|       184|  213.4608779332297|\n",
      "|       188| 211.31431451612903|\n",
      "|       126|               NULL|\n",
      "|       149| 162.22487810409342|\n",
      "|        94|               NULL|\n",
      "|        50|  174.6398937784522|\n",
      "|       190|  227.1566903684551|\n",
      "|       110|               NULL|\n",
      "|       136|               NULL|\n",
      "|        57| 165.59751158534212|\n",
      "|       144| 179.55416681718023|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myresults = sp.sql(\"SELECT station_id, AVG(solar_radiation) AS avg_solar_radiation FROM weather GROUP BY station_id\")\n",
    "myresults.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d29693-4abd-4a5d-bbe0-47648726e95b",
   "metadata": {},
   "source": [
    "This finds the average solar radiation in every station in watts per square meter (W/M^2). This could tell us a lot of things. First, we could hypothesis about the cloud coverage. the more clouds, the less solar radiation because the clouds are reflecting the light. Second, perhaps this is at a higher elevation, so it gets more sunlight. This is why I look at average pressure later\n",
    "\n",
    "Just from looking at this table, I can hypothesis that stations 190 and 191 are either higher in elevation, get less cloud coverage, which could also tell us about their location without it being told to us, or it's a mix of both :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f679544-46d4-459a-99a9-867a393889d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|station_id|      avg_pressure|\n",
      "+----------+------------------+\n",
      "|       191|30.066801111953573|\n",
      "|        65|  30.0257834175469|\n",
      "|        54| 30.24833401542529|\n",
      "|       167| 29.19178063859176|\n",
      "|       155|29.369080087385147|\n",
      "|       112|30.125069594722362|\n",
      "|       113| 29.89896177278859|\n",
      "|       130|29.890234047861878|\n",
      "|        77|30.007343742262762|\n",
      "|       184|29.912333111259198|\n",
      "|       188|29.983693447580652|\n",
      "|       126|30.039251707063215|\n",
      "|       149|29.323806281891507|\n",
      "|        94|30.003139923508776|\n",
      "|        50| 29.38559667678274|\n",
      "|       190| 30.08626800258568|\n",
      "|       110|30.045749364720386|\n",
      "|       136|29.988627200691703|\n",
      "|        57|30.303463675541455|\n",
      "|       144|29.303840918075966|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myresults = sp.sql(\"SELECT station_id, AVG(pressure) AS avg_pressure FROM weather GROUP BY station_id\")\n",
    "myresults.show() # in inches Hg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf00fb-2418-460f-ae94-2a32b0c28952",
   "metadata": {},
   "source": [
    "This finds the average pressure for every station in inches of mercury (in Hg). Looking at the avergae pressure is a good way to tell the height of the stations. Higher pressure means lower elevation. Since many of the stations have similar average pressure, this tells me that stations 190 and 191 have less cloud coverage than the other stations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76442d6-d02e-4f26-b9d6-c3183e1d6929",
   "metadata": {},
   "source": [
    "## Pledge\n",
    "\n",
    "By submitting this work I hereby pledge that this is my own, personal work. I've acknowledged in the designated place at the top of this file all sources that I used to complete said work, including but not limited to: online resources, books, and electronic communications. I've noted all collaboration with fellow students and/or TA's. I did not copy or plagiarize another's work.\n",
    "\n",
    "> As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do. Accountable together – We are Purdue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "seminar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
